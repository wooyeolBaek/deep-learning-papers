# Basic DL

<!-- 1. [CNN](#1-cnn)
2. [Transformers](#2-transformers) -->

<!-- > ## 1. CNN -->
<!-- ### Very Deep Convolutional Networks for Large-Scale Image Recognition
- [VGG](https://maize-skink-ffe.notion.site/Very-Deep-Convolutional-Networks-for-Large-Scale-Image-Recognition-0f9110115ad14835843b20ec18180006) -->
<!-- ### Going Deeper with Convolutions
- [InceptionNet(GoogLeNet)](https://maize-skink-ffe.notion.site/Going-Deeper-with-Convolutions-08c04b7ca5e64386bc7ba1edeb7dd81a) -->
<!-- ### U-Net: Convolutional Networks for Biomedical Image Segmentation
- [UNet](https://maize-skink-ffe.notion.site/U-Net-Convolutional-Networks-for-Biomedical-Image-Segmentation-9ebe8c667c3d4e0da00e02bd4579c16d) -->
<!-- ### Rethinking the Inception Architecture for Computer Vision
- InceptionNet v2, v3 -->
<!-- ### Deep Residual Learning for Image Recognition
- [ResNet](https://maize-skink-ffe.notion.site/Deep-Residual-Learning-for-Image-Recognition-9c88e8683baa4f87916c4e07ae3420f1) -->
<!-- ### Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning
- InceptionNet v4 -->
<!-- ### Indentity Mapping in Deep Residual Networks
- Improved ResNet(full pre-activation) -->
<!-- ### Residual Networks Behave Like Ensembles of Relatively Shallow Networks
- Analyzed ResNet -->
<!-- ### Wide Residual Networks
- Wide ResNet -->
<!-- ### Densely Connected Convolutional Networks
- DenseNet -->
<!-- ### Aggregated Residual Transformations for Deep Neural Networks
- ResNeXt -->
<!-- ### MobileNets: Dfficient Convolutional Neural Networks for Mobile Vision Applications
- MobileNet v1 -->
<!-- ### MovbileNetV2: Inverted Residuals and Linear Bottlenecks
- MobileNet v2 -->
<!-- ### Implicit Generation and Generalization in Energy-Based Models
- MobileNet v3 -->
<!-- ### EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks
- EfficientNet -->
<!-- ### A ConvNet for the 2020s
- ConvNext -->
<!-- ### ConvNeXt V2: Co-designing and Scaling ConvNets with Masked Autoencoders
- ConvNeXt v2 -->



<!-- > ## 2. Transformers -->
> ## Transformers
### An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale
- [ViT](https://www.notion.so/An-Image-is-Worth-16x16-Words-Transformers-for-Image-Recognition-at-Scale-c9a6a885bbdf4cb096b1a10680c6e708?pvs=25)
<!-- > ## Normalizations -->
<!-- ### Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift
- [Batch Norm: (N, C, H, W)에서 N에 대해 normalize](https://www.notion.so/Batch-Normalization-Accelerating-Deep-Network-Training-by-Reducing-Internal-Covariate-Shift-c2e2ac6513254d928dd84d07496c057) -->
<!-- ### Layer Normalization
- [Layer Norm: (N, *)에서 *에 대해 normalize](https://www.notion.so/Layer-normalization-9196fa75aaf84b4481c6ec2be83d4bea) -->
<!-- ### Instance Normalization: The Missing Ingredient for Fast Stylization
- [Instance Norm: (N, C, *)에서 *에 대해 normalize](https://www.notion.so/Instance-Normalization-The-Missing-Ingredient-for-Fast-Stylization-610bd1c2433d44a0b5d41ddc31b729da) -->
<!-- ### Group Normalization
- [Group Norm: (N, C, *)에서 C'와 *에 대해 normalize](https://www.notion.so/Group-Normalization-d75e7eb393254e94af9cb7c30460be64) -->

<!-- > ## ETC -->
<!-- ### Learning deep features for discriminative localization
- Localization -->
<!-- ### Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization
- Localization, XAI -->
<!-- ### Visualizing the Loss Landscape of Neural Nets
- Loss Landscape -->
<!-- ### AutoML: A Survey of the State-of-the-Art
- Auto-ML -->

> ## Parameter Efficient Tuning
### LoRA: Low-Rank Adaptation of Large Language Models
- [LoRA](https://www.notion.so/LoRA-Low-Rank-Adaptation-of-Large-Language-Models-f7b902a9696e4116a682c85a01a96115)

