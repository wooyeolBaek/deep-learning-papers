# Multimodal

# Vision-Language
<!-- ### VQA: Visual Question Answering
- VQA -->
### Learning Transferable Visual Models From Natural Language Supervision
- [CLIP](https://www.notion.so/Learning-Transferable-Visual-Models-From-Natural-Language-Supervision-5953f2cb56e54ab88a507296c3a93be8)
<!-- ### Repreducible scaling laws for contrastive language-image learning
- OpenCLIPI-ViT/G -->
### ImageBind: ImageBind: One Embedding Space To Bind Them All
- [ImageBind](https://www.notion.so/ImageBind-One-Embedding-Space-To-Bind-Them-All-7ebc92f668584e7592223d958233dcd3?pvs=25)
### Visual Instruction Tuning
- [LLaVA](https://www.notion.so/Visual-Instruction-Tuning-af699986382845f1be7f34df2aa415a4?pvs=25)
### Grounding Language Models to Images for Multimodal Inputs and Outputs
- [LLM의 입력에 visual embedding 역할을 하는 [RET] 토큰을 추가해 이미지 검색(retrieval)이 가능하도록 확장](https://maize-skink-ffe.notion.site/Grounding-Language-Models-to-Images-for-Multimodal-Inputs-and-Outputs-517f1c27943741e8b1200cafba7d1a2d)
<!-- ### Generating Images with Multimodal Language Models
- [GILL:](https://maize-skink-ffe.notion.site/Generating-Images-with-Multimodal-Language-Models-97247a649ea7444a999ba507d6a644c0?pvs=74) -->
<!-- ### ImageBind-LLM: Multi-modality Instruction Tuning
- [ImageBInd-LLM](https://maize-skink-ffe.notion.site/ImageBind-LLM-Multi-modality-Instruction-Tuning-20cd9cceb0c8454aa9b44c7ac0fbb8f7) -->