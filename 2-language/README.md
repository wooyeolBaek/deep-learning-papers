# language

<!-- ### Efficient Estimation of Word Representations in Vector Space
- Word2Vec -->
<!-- ### Learning Phrase Representation using RNN Encoder-Decoder for Statistical Machine Translation
- RNN Encoder-Decoder -->
<!-- ### Sequence to Sequence Learning with Neural Networks
- Seq2Seq -->
<!-- ### Neural Machine Translation by Jointly Learning to Align and Translate
- Bahdanau Attention -->
<!-- ### Effective Approaches to Attention-based Neural Machine Translation
- Luong Attention -->
<!-- ### Enriching Word Vectors with Subword Information
- FastText -->
<!-- ### Deep contextualized word representations
- ELMo -->
<!-- ### GloVe: Global Nectors for Aord Representation
- GloVe -->
<!-- ### LSTM: A Search Space Odyssey
- LSTM -->
### Attention is all you need
- [Transformer](https://www.notion.so/Attention-Is-All-You-Need-e84595ff29784915a403b26c5ed3449f?pvs=25)
### Improving Language Understanding by Generative Pre-Training
- [GPT1, 현재 토큰으로 다음 토큰 예측하는 방식](https://www.notion.so/Improving-Language-Understanding-by-Generative-Pre-Training-4df86d8fcde240178248d016db93ad2c)
<!-- ### Glow: Generative Flow with Invertible 1x1 Convolutions
- Glow -->
<!-- ### BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding
- BERT -->
### Language Models are Unsupervised Multitask Learners
- [GPT2, In-Context Learning, Zero-shot](https://www.notion.so/Language-Models-are-Unsupervised-Multitask-Learners-3217f397cd8e4eefbaf4b7890ca6606d)
### Language Models are Few-Shot Learners
- [GPT3, In-Context Learning, Zero/One/Few-shot input sequence](https://www.notion.so/Language-Models-are-Few-Shot-Learners-96833fe3e98b40258bea13d311cffb96)
### Reinforcement Learning with Human Feedback: Learning Dynamic Choices via Pessimism
- [InstructGPT, RLHF](https://www.notion.so/Training-language-models-to-follow-instructions-with-human-feedback-dfa45c7948004f6a80ae3246f16433d2?pvs=25)
<!-- ### Direct Preference Optimization: Your Language Model is Secretly a Reward Model
- [DPO: Reward 모델 없이 LLM 파인튜닝하는 방법](https://www.notion.so/Direct-Preference-Optimization-Your-Language-Model-is-Secretly-a-Reward-Model-2485aee85a2c48069e6bc7e9e0692eed) -->
