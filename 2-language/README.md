# language

<!-- ### Efficient Estimation of Word Representations in Vector Space
- Word2Vec -->
<!-- ### Learning Phrase Representation using RNN Encoder-Decoder for Statistical Machine Translation
- RNN Encoder-Decoder -->
<!-- ### Sequence to Sequence Learning with Neural Networks
- Seq2Seq -->
<!-- ### Neural Machine Translation by Jointly Learning to Align and Translate
- Bahdanau Attention -->
<!-- ### Effective Approaches to Attention-based Neural Machine Translation
- Luong Attention -->
<!-- ### Enriching Word Vectors with Subword Information
- FastText -->
<!-- ### Deep contextualized word representations
- ELMo -->
<!-- ### GloVe: Global Nectors for Aord Representation
- GloVe -->
<!-- ### LSTM: A Search Space Odyssey
- LSTM -->
### Attention is all you need
- Transformer
<!-- ### Improving Language Understanding by Generative Pre-Training
- GPT1 -->
<!-- ### Glow: Generative Flow with Invertible 1x1 Convolutions
- Glow -->
<!-- ### BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding
- BERT -->
### LoRA: Low-Rank Adaptation of Large Language Models
- LoRA
<!-- ### Reinforcement Learning with Human Feedback: Learning Dynamic Choices via Pessimism
- RLHF -->
<!-- ### Direct Preference Optimization: Your Language Model is Secretly a Reward Model
- DPO: Reward 모델 없이 LLM 파인튜닝하는 방법 -->
